{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tf_spec2_week_4_1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0EtpJSTyPJ6"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFvRQwnsy05x",
        "outputId": "4cae4496-dd72-47cb-8b77-0971dca21fe8"
      },
      "source": [
        "#TPU strategy\n",
        "\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version \" + tf.__version__)\n",
        "AUTO = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version 2.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y6h7rL2PB9hz"
      },
      "source": [
        "# pronalazenje harvera\n",
        "\n",
        "try:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR'] #mora biti ukljucen TPU\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address) # TPU detection\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.experimental.TPUStrategy(tpu) \n",
        "  # Going back and forth between TPU and host is expensive.\n",
        "  # Better to run 128 batches on the TPU before reporting back.\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \n",
        "  print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "except ValueError:\n",
        "  print('TPU failed to initialize.')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iShz-_KmCCS3"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}